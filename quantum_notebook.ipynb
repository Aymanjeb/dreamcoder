{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import dreamcoder as dc\n",
    "from dreamcoder.domains.quantum_algorithms.primitives import *\n",
    "from dreamcoder.domains.quantum_algorithms.tasks import *\n",
    "\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing some circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_qubit = 2\n",
    "full_circuit = [n_qubit,\n",
    "           [[\"cnot\", 0, 1],\n",
    "           [\"swap\", 0, 1],\n",
    "           [\"hadamard\", 1]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = eye(n_qubit)\n",
    "tensor_to_mat(swap(cnot(tensor,0,1),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.707,  0.707,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.707,  0.707],\n",
       "       [ 0.   ,  0.   ,  0.707, -0.707],\n",
       "       [ 0.707, -0.707,  0.   ,  0.   ]], dtype=float16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_circuit_to_mat(full_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \n",
      "q_0: ──■───X──────\n",
      "     ┌─┴─┐ │ ┌───┐\n",
      "q_1: ┤ X ├─X─┤ H ├\n",
      "     └───┘   └───┘\n"
     ]
    }
   ],
   "source": [
    "print_circuit(full_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌───┐   ┌───┐\n",
      "q_0: ┤ X ├─X─┤ H ├\n",
      "     └─┬─┘ │ └───┘\n",
      "q_1: ──■───X──────\n",
      "                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Code consistent with Qiskit\n"
     ]
    }
   ],
   "source": [
    "with QiskitTester(full_circuit) as QT:\n",
    "    QT.circuit.cnot(QT.q(0),QT.q(1))\n",
    "    QT.circuit.swap(QT.q(0),QT.q(1))\n",
    "    QT.circuit.h(QT.q(1))\n",
    "print(QT)\n",
    "QT.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -1, 3], [['cnot', 1, 0]]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubit= 3\n",
    "code = dc.program.Program.parse(\"(lambda (cnot (minv(mv(no_op $0)))))\")\n",
    "code.infer()\n",
    "code.evaluate([])(n_qubit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_circuit_to_mat(code.evaluate([])(n_qubit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing some Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = makeTasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -3.8918202981106265)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"hadamard_0\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (h (no_op $0)))\")\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -3.8918202981106265)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task =get_task_from_name(\"cnot_01\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (cnot (no_op $0)))\")\n",
    "task.logLikelihood(code), grammar.logLikelihood(code.infer(), code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -7.783640596221253)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"cnot_10\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (cnot (minv(mv(no_op $0)))))\")\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -15.567281192442506)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"swap_01\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda  (cnot(minv(mv_r(cnot(minv (mv (cnot (no_op $0)))))))))\")\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., -1.]], dtype=float16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"cz_01\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (h(mv(cnot(mv_r(h (mv (no_op $0))))))))\")\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n",
    "np.round(state_circuit_to_mat(code.evaluate([])(2)),decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "q_0: ───\n",
      "        \n",
      "q_1: ─■─\n",
      "      │ \n",
      "q_2: ─■─\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Code consistent with Qiskit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0., -0., -0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1., -0., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0., -1., -0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0., -0., -1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with QiskitTester(code.evaluate([])(3)) as QT:\n",
    "    QT.circuit.cz(QT.q(0),QT.q(1))\n",
    "print(QT)\n",
    "QT.check()\n",
    "np.real(np.array(QT.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "q_0: ─■─\n",
      "      │ \n",
      "q_1: ─■─\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., -0.],\n",
       "       [ 0.,  1.,  0., -0.],\n",
       "       [ 0.,  0.,  1., -0.],\n",
       "       [ 0.,  0.,  0., -1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with QiskitTester(code.evaluate([])(2)) as QT:\n",
    "    QT.circuit.cz(QT.q(1),QT.q(0))\n",
    "print(QT)\n",
    "np.real(np.array(QT.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -14.155496613885283)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"cnot_nn_1\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (cnot ((rep (dec(dec(size_to_int $0))) (lambda (mv $0))) (no_op $0))))\")\n",
    "code.evaluate([])(3)\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "          ┌───┐                         ┌───┐     \n",
      "q_0: ──■──┤ X ├──■───────────────────■──┤ X ├──■──\n",
      "     ┌─┴─┐└─┬─┘┌─┴─┐     ┌───┐     ┌─┴─┐└─┬─┘┌─┴─┐\n",
      "q_1: ┤ X ├──■──┤ X ├──■──┤ X ├──■──┤ X ├──■──┤ X ├\n",
      "     └───┘     └───┘┌─┴─┐└─┬─┘┌─┴─┐└───┘     └───┘\n",
      "q_2: ───────────────┤ X ├──■──┤ X ├───────────────\n",
      "                    └───┘     └───┘               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, -52.145060152057745)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"swap_0n\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda ((  (rep (dec(dec(size_to_int $0))) (lambda ((cnot(minv(mv_r(cnot(minv (mv (cnot(mv_r $0)))))))))) )  (mv_r( (rep (dec(size_to_int $0)) (lambda (mv((cnot(minv(mv_r(cnot(minv (mv (cnot $0)))))))))) ) (no_op $0) )))))\")\n",
    "print_circuit(code.evaluate([])(3))\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If swap was included\n",
    "# task = get_task_from_name(\"swap_0n\",tasks)\n",
    "# code = dc.program.Program.parse(\"(lambda ((  (rep (dec(dec(size_to_int $0))) (lambda (swap(mv_r $0))) )  (mv_r( (rep (dec(size_to_int $0)) (lambda (mv(swap $0))) ) (no_op $0) )))))\")\n",
    "# print_circuit(code.evaluate([])(5))\n",
    "# task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If swap was included\n",
    "# task = get_task_from_name(\"swap_0n\",tasks)\n",
    "# code = dc.program.Program.parse(\"(lambda ((  (rep (dec(dec(size_to_int $0))) (lambda (swap(mv_r $0))) )  (mv_r( (rep (dec(size_to_int $0)) (lambda (mv(swap $0))) ) (no_op $0) )))))\")\n",
    "# print_circuit(code.evaluate([])(5))\n",
    "# task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile bottom-up enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available?: False\n",
      "using cuda?: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/Users/lsarra/opt/anaconda3/envs/dc/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "usage: ipykernel_launcher.py [-h] [--resume RESUME] [-i ITERATIONS]\n",
      "                             [-t ENUMERATIONTIMEOUT] [-R RECOGNITIONTIMEOUT]\n",
      "                             [-RS RECOGNITIONSTEPS] [-k TOPK]\n",
      "                             [-p PSEUDOCOUNTS] [-b AIC] [-l STRUCTUREPENALTY]\n",
      "                             [-a ARITY] [-c CPUS] [--no-cuda]\n",
      "                             [-m MAXIMUMFRONTIER] [--reuseRecognition]\n",
      "                             [--recognition] [--ensembleSize ENSEMBLESIZE]\n",
      "                             [-g] [-d] [--no-consolidation]\n",
      "                             [--testingTimeout TESTINGTIMEOUT]\n",
      "                             [--testEvery TESTEVERY] [--seed SEED]\n",
      "                             [--activation {relu,sigmoid,tanh}]\n",
      "                             [--solver {ocaml,pypy,bottom,python}]\n",
      "                             [-r HELMHOLTZRATIO]\n",
      "                             [--compressor {pypy,rust,vs,pypy_vs,ocaml,memorize}]\n",
      "                             [--matrixRank MATRIXRANK] [--mask]\n",
      "                             [--biasOptimal] [--contextual]\n",
      "                             [--clear-recognition CLEAR-RECOGNITION]\n",
      "                             [--primitive-graph PRIMITIVE-GRAPH [PRIMITIVE-GRAPH ...]]\n",
      "                             [--taskBatchSize TASKBATCHSIZE]\n",
      "                             [--taskReranker {default,random,randomShuffle,unsolved,unsolvedEntropy,unsolvedRandomEntropy,randomkNN,randomLowEntropykNN}]\n",
      "                             [--storeTaskMetrics] [--rewriteTaskMetrics]\n",
      "                             [--addTaskMetrics ADDTASKMETRICS [ADDTASKMETRICS ...]]\n",
      "                             [--auxiliary] [--addFullTaskMetrics]\n",
      "                             [--countParameters COUNTPARAMETERS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"0a0f4965-9892-404e-adcd-73854468aa16\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=/var/folders/g6/m3rq3pbs7lq6drdpnnfm1fvjwthg2w/T/tmp-8431LxyAicQH9lzp.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import binutil  # required to import from dreamcoder modules\n",
    "except ModuleNotFoundError:\n",
    "    import bin.binutil  # alt import if called as module\n",
    "\n",
    "from dreamcoder.domains.quantum_algorithms.main import main\n",
    "from dreamcoder.dreamcoder import commandlineArguments\n",
    "from dreamcoder.utilities import numberOfCPUs\n",
    "\n",
    "arguments = commandlineArguments(\n",
    "    featureExtractor=None, # it was TowerCNN\n",
    "    CPUs=numberOfCPUs(),\n",
    "    helmholtzRatio=0.5,\n",
    "    recognitionTimeout=6,\n",
    "    iterations=6,\n",
    "    a=3,\n",
    "    structurePenalty=1,\n",
    "    pseudoCounts=10,\n",
    "    topK=2,\n",
    "    maximumFrontier=5,\n",
    "    extras=None,\n",
    "    solver=\"python\", \n",
    "    useRecognitionModel=False,\n",
    "    enumerationTimeout=6,#-g\n",
    "    compressor=\"pypy\")   #ocaml, python, pypy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running EC on 01-mar-grp-0020 @ 2022-03-25 12:32:35.820498 with 8 CPUs and parameters:\n",
      "\t noConsolidation  =  False\n",
      "\t iterations  =  6\n",
      "\t enumerationTimeout  =  6\n",
      "\t useRecognitionModel  =  False\n",
      "\t topk_use_only_likelihood  =  False\n",
      "\t pseudoCounts  =  10\n",
      "\t aic  =  1.0\n",
      "\t structurePenalty  =  1\n",
      "\t arity  =  3\n",
      "\t taskReranker  =  default\n",
      "\t storeTaskMetrics  =  True\n",
      "\t rewriteTaskMetrics  =  False\n",
      "\t maximumFrontier  =  5\n",
      "\t solver  =  python\n",
      "\t topK  =  2\n",
      "\t evaluationTimeout  =  0.01\n",
      "\t cuda  =  False\n",
      "\n",
      "Currently using this much memory: 225517568\n",
      "Currently using this much memory: 225517568\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.966272.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.933642.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.888042.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.754496.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.417176.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 4.360445.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 1.203866.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [168, 168, 869, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.891820 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.891820 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.783641 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 5.189094 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 1\n",
      "Currently using this much memory: 225927168\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 225927168\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.28\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-4.17\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "-4.17\t(lambda (cnot (h (h (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.89\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.89\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.83\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.83\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "-2.83\t(lambda (mv_r (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -15.567281\tNew joint = -14.998451\n",
      "\n",
      "1.202724 / 8.778653\tmv\n",
      "0.052910 / 8.778653\tmv_r\n",
      "1.425419 / 8.778653\tminv\n",
      "3.000000 / 8.778653\tno_op\n",
      "1.029770 / 8.778653\th\n",
      "2.067830 / 8.778653\tcnot\n",
      "0.000000 / 8.778653\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.3 seconds\n",
      "Grammar after iteration 1:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 225865728\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=1_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_0_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_0_unordered.pdf\n",
      "Currently using this much memory: 225714176\n",
      "Currently using this much memory: 225714176\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.971279.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.943310.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.898743.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.803303.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.686891.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 5.128656.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 2.464150.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [202, 202, 1119, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092, 2092]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.756973 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.669962 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.571516 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 4.999484 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 0\n",
      "Currently using this much memory: 226070528\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.506977\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (minv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (h (minv (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.756973\n",
      "WARNING: \tThe program is (lambda (h (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.718987\n",
      "WARNING: \tThe program is (lambda (mv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.669962\n",
      "WARNING: \tThe program is (lambda (cnot (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.631975\n",
      "WARNING: \tThe program is (lambda (mv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.419966\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.609502\n",
      "WARNING: \tThe program is (lambda (minv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (mv (minv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (minv (mv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 226070528\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.29\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-3.94\t(lambda (cnot (minv (minv (no_op $0)))))\n",
      "-4.18\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.90\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.90\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.65\t(lambda (mv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -14.998451\tNew joint = -14.998451\n",
      "\n",
      "1.260546 / 8.784187\tmv\n",
      "0.000000 / 8.784187\tmv_r\n",
      "1.455835 / 8.784187\tminv\n",
      "3.000000 / 8.784187\tno_op\n",
      "1.000000 / 8.784187\th\n",
      "2.067806 / 8.784187\tcnot\n",
      "0.000000 / 8.784187\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.7 seconds\n",
      "Grammar after iteration 2:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 225996800\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=2_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_1_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_1_unordered.pdf\n",
      "Currently using this much memory: 225853440\n",
      "Currently using this much memory: 225853440\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.965786.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.932314.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.880273.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.775636.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.634202.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 4.900280.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 2.251888.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [202, 202, 1119, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069, 2069]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.756973 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.669962 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.571516 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 4.999484 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 0\n",
      "Currently using this much memory: 225558528\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.506977\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (minv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (h (minv (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.756973\n",
      "WARNING: \tThe program is (lambda (h (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.718987\n",
      "WARNING: \tThe program is (lambda (mv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.669962\n",
      "WARNING: \tThe program is (lambda (cnot (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.631975\n",
      "WARNING: \tThe program is (lambda (mv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.419966\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.609502\n",
      "WARNING: \tThe program is (lambda (minv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (mv (minv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (minv (mv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 225558528\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.29\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-3.94\t(lambda (cnot (minv (minv (no_op $0)))))\n",
      "-4.18\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.90\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.90\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.65\t(lambda (mv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -14.998451\tNew joint = -14.998451\n",
      "\n",
      "1.260546 / 8.784187\tmv\n",
      "0.000000 / 8.784187\tmv_r\n",
      "1.455835 / 8.784187\tminv\n",
      "3.000000 / 8.784187\tno_op\n",
      "1.000000 / 8.784187\th\n",
      "2.067806 / 8.784187\tcnot\n",
      "0.000000 / 8.784187\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.6 seconds\n",
      "Grammar after iteration 3:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 225497088\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=3_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_2_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_2_unordered.pdf\n",
      "Currently using this much memory: 225415168\n",
      "Currently using this much memory: 225415168\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.970709.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.916057.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.866513.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.775666.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.648860.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 5.043345.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 2.101726.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [202, 202, 1119, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.756973 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.669962 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.571516 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 4.999484 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 0\n",
      "Currently using this much memory: 225435648\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.506977\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (minv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (h (minv (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.756973\n",
      "WARNING: \tThe program is (lambda (h (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.718987\n",
      "WARNING: \tThe program is (lambda (mv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.669962\n",
      "WARNING: \tThe program is (lambda (cnot (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.631975\n",
      "WARNING: \tThe program is (lambda (mv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.419966\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.609502\n",
      "WARNING: \tThe program is (lambda (minv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (mv (minv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (minv (mv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 225435648\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.29\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-3.94\t(lambda (cnot (minv (minv (no_op $0)))))\n",
      "-4.18\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.90\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.90\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.65\t(lambda (mv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -14.998451\tNew joint = -14.998451\n",
      "\n",
      "1.260546 / 8.784187\tmv\n",
      "0.000000 / 8.784187\tmv_r\n",
      "1.455835 / 8.784187\tminv\n",
      "3.000000 / 8.784187\tno_op\n",
      "1.000000 / 8.784187\th\n",
      "2.067806 / 8.784187\tcnot\n",
      "0.000000 / 8.784187\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.3 seconds\n",
      "Grammar after iteration 4:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 225370112\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=4_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_3_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_3_unordered.pdf\n",
      "Currently using this much memory: 225239040\n",
      "Currently using this much memory: 225239040\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.973917.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.946072.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.905853.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.816774.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.683275.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 5.117403.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 2.566357.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [202, 202, 1119, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342, 2342]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.756973 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.669962 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.571516 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 4.999484 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 0\n",
      "Currently using this much memory: 225259520\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.506977\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (minv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (h (minv (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.756973\n",
      "WARNING: \tThe program is (lambda (h (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.718987\n",
      "WARNING: \tThe program is (lambda (mv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.669962\n",
      "WARNING: \tThe program is (lambda (cnot (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.631975\n",
      "WARNING: \tThe program is (lambda (mv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.419966\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.609502\n",
      "WARNING: \tThe program is (lambda (minv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (mv (minv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (minv (mv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 225259520\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.29\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-3.94\t(lambda (cnot (minv (minv (no_op $0)))))\n",
      "-4.18\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.90\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.90\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.65\t(lambda (mv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -14.998451\tNew joint = -14.998451\n",
      "\n",
      "1.260546 / 8.784187\tmv\n",
      "0.000000 / 8.784187\tmv_r\n",
      "1.455835 / 8.784187\tminv\n",
      "3.000000 / 8.784187\tno_op\n",
      "1.000000 / 8.784187\th\n",
      "2.067806 / 8.784187\tcnot\n",
      "0.000000 / 8.784187\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.3 seconds\n",
      "Grammar after iteration 5:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 225193984\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=5_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_4_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_4_unordered.pdf\n",
      "Currently using this much memory: 225054720\n",
      "Currently using this much memory: 225054720\n",
      "Using a waking task batch of size: 19\n",
      "Disabling parallelism on the Python side because we only have one job.\n",
      "If you are using ocaml or bottom, there could still be parallelism.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 0.000000 <= MDL < 1.500000. Timeout 6.000000.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 1.500000 <= MDL < 3.000000. Timeout 5.971404.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 3.000000 <= MDL < 4.500000. Timeout 5.944928.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 4.500000 <= MDL < 6.000000. Timeout 5.907639.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 6.000000 <= MDL < 7.500000. Timeout 5.822110.\n",
      "(frontend) Launching tsize -> tcircuit (19 tasks) w/ 8 CPUs. 7.500000 <= MDL < 9.000000. Timeout 5.702229.\n",
      "(frontend) Launching tsize -> tcircuit (17 tasks) w/ 8 CPUs. 9.000000 <= MDL < 10.500000. Timeout 5.167367.\n",
      "(frontend) Launching tsize -> tcircuit (16 tasks) w/ 8 CPUs. 10.500000 <= MDL < 12.000000. Timeout 2.800816.\n",
      "We enumerated this many programs, for each task:\n",
      "\t [202, 202, 1119, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321, 2321]\n",
      "Generative model enumeration results:\n",
      "HIT hadamard_0 w/ (lambda (h (no_op $0))) ; log prior = -3.756973 ; log likelihood = 0.000000\n",
      "HIT cnot_01 w/ (lambda (cnot (no_op $0))) ; log prior = -3.669962 ; log likelihood = 0.000000\n",
      "HIT cnot_10 w/ (lambda (cnot (minv (mv (no_op $0))))) ; log prior = -7.571516 ; log likelihood = 0.000000\n",
      "MISS cnot_02\n",
      "MISS cnot_20\n",
      "MISS swap_01\n",
      "MISS swap_02\n",
      "MISS swap_12\n",
      "MISS cz_01\n",
      "MISS cz_12\n",
      "MISS cz_02\n",
      "MISS hadamard_n\n",
      "MISS hadamard_n_1\n",
      "MISS cnot_nn_1\n",
      "MISS swap_nn_1\n",
      "MISS cz_nn_1\n",
      "MISS swap_0n\n",
      "MISS swap_0n_1\n",
      "MISS cnot_0n\n",
      "Hits 3/19 tasks\n",
      "Average description length of a program solving a task: 4.999484 nats\n",
      "Generative model average:  0 sec.\tmedian: 0 \tmax: 1 \tstandard deviation 0\n",
      "Currently using this much memory: 225075200\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.506977\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (minv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.696514\n",
      "WARNING: \tThe program is (lambda (h (minv (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.756973\n",
      "WARNING: \tThe program is (lambda (h (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.718987\n",
      "WARNING: \tThe program is (lambda (mv (h (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -3.891820 vs -3.669962\n",
      "WARNING: \tThe program is (lambda (cnot (no_op $0)))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.631975\n",
      "WARNING: \tThe program is (lambda (mv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.419966\n",
      "WARNING: \tThe program is (lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -5.837730 vs -5.609502\n",
      "WARNING: \tThe program is (lambda (minv (cnot (no_op $0))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (mv (minv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -7.783641 vs -7.571516\n",
      "WARNING: \tThe program is (lambda (cnot (minv (mv (no_op $0)))))\n",
      "\n",
      "WARNING: Log priors differed during frontier combining: -9.729551 vs -9.511057\n",
      "WARNING: \tThe program is (lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "Frontiers discovered top down: 3\n",
      "Total frontiers: 3\n",
      "Currently using this much memory: 225075200\n",
      "Showing the top 5 programs in each frontier being sent to the compressor:\n",
      "hadamard_0\n",
      "-0.37\t(lambda (h (no_op $0)))\n",
      "-2.32\t(lambda (h (minv (no_op $0))))\n",
      "-2.32\t(lambda (minv (h (no_op $0))))\n",
      "-2.32\t(lambda (mv (h (no_op $0))))\n",
      "-4.26\t(lambda (cnot (cnot (h (no_op $0)))))\n",
      "\n",
      "cnot_01\n",
      "-0.29\t(lambda (cnot (no_op $0)))\n",
      "-2.23\t(lambda (minv (cnot (no_op $0))))\n",
      "-2.23\t(lambda (mv (cnot (no_op $0))))\n",
      "-3.94\t(lambda (cnot (minv (minv (no_op $0)))))\n",
      "-4.18\t(lambda (cnot (cnot (cnot (no_op $0)))))\n",
      "\n",
      "cnot_10\n",
      "-0.90\t(lambda (cnot (minv (mv (no_op $0)))))\n",
      "-0.90\t(lambda (cnot (mv (minv (no_op $0)))))\n",
      "-2.65\t(lambda (mv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (minv (mv (no_op $0))))))\n",
      "-2.85\t(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Failure loading recognition - only acceptable if using pypy \n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import recognition. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "WARNING: Could not import torch. This is only okay when doing pypy compression.\n",
      "Inducing a grammar from 3 frontiers\n",
      "Starting score -36.99845058915432\n",
      "Proposed 10 fragments.\n",
      "Old joint = -14.998451\tNew joint = -14.998451\n",
      "\n",
      "1.260546 / 8.784187\tmv\n",
      "0.000000 / 8.784187\tmv_r\n",
      "1.455835 / 8.784187\tminv\n",
      "3.000000 / 8.784187\tno_op\n",
      "1.000000 / 8.784187\th\n",
      "2.067806 / 8.784187\tcnot\n",
      "0.000000 / 8.784187\trep\n",
      "0.000000 / 0.000000\t0\n",
      "0.000000 / 0.000000\tinc\n",
      "0.000000 / 0.000000\tdec\n",
      "0.000000 / 0.000000\tsize_to_int\n",
      "Induced a grammar in 1.6 seconds\n",
      "Grammar after iteration 6:\n",
      "1.466337\tt0\t$_\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n",
      "-0.339216\ttsize -> tcircuit\tno_op\n",
      "-0.419258\ttcircuit -> tcircuit\tcnot\n",
      "-0.483797\ttcircuit -> tcircuit\tminv\n",
      "-0.506270\ttcircuit -> tcircuit\tmv\n",
      "-0.506270\ttcircuit -> tcircuit\th\n",
      "-0.601580\ttcircuit -> tcircuit\tmv_r\n",
      "-0.601580\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "Currently using this much memory: 224989184\n",
      "Exported checkpoint to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_aic=1.0_arity=3_ET=6_it=6_MF=5_noConsolidation=False_pc=10_RW=False_solver=python_STM=True_L=1_TRR=default_K=2_topkNotMAP=False_rec=False.pickle\n",
      "Exporting primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_5_depth.pdf\n",
      "Exported primitive graph to experimentOutputs/quantum/2022-03-25T12:32:35.806330/quantum_primitives_5_unordered.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 1.06798 s\n",
      "File: /Users/lsarra/ownCloud/topics/artificial-scientific-discovery/2021_Unitary-Synthesis/ec/dreamcoder/domains/quantum_algorithms/primitives.py\n",
      "Function: tensor_contraction at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def tensor_contraction(A, B, indices):\n",
      "    45      8907      18291.0      2.1      1.7      n_qubits = get_qubit_number(A)\n",
      "    46      8907      18792.0      2.1      1.8      idx = [i + n_qubits for i in indices]\n",
      "    47      8907     774326.0     86.9     72.5      out = np.tensordot(A, B, (idx, np.arange(len(indices))))\n",
      "    48      8907     256571.0     28.8     24.0      return np.moveaxis(out, np.arange(-len(indices), 0, 1), idx)\n",
      "\n",
      "Total time: 1.31214 s\n",
      "File: /Users/lsarra/ownCloud/topics/artificial-scientific-discovery/2021_Unitary-Synthesis/ec/dreamcoder/domains/quantum_algorithms/primitives.py\n",
      "Function: full_circuit_to_mat at line 129\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   129                                           def full_circuit_to_mat(full_circuit):\n",
      "   130      6181       3203.0      0.5      0.2      n_qubit, op_list = full_circuit\n",
      "   131                                               \n",
      "   132      6181     138071.0     22.3     10.5      tensor = eye(n_qubit)\n",
      "   133     15088       9797.0      0.6      0.7      for op in op_list:\n",
      "   134                                                   \n",
      "   135      8907    1126483.0    126.5     85.9          tensor = full_op_names[op[0]](tensor, *op[1:])\n",
      "   136                                                   \n",
      "   137      6181      34585.0      5.6      2.6      return tensor_to_mat(tensor)\n",
      "\n",
      "Total time: 2.70419 s\n",
      "File: /Users/lsarra/ownCloud/topics/artificial-scientific-discovery/2021_Unitary-Synthesis/ec/dreamcoder/domains/quantum_algorithms/primitives.py\n",
      "Function: execute_quantum_algorithm at line 425\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   425                                           def execute_quantum_algorithm(p, n_qubits, timeout=None):\n",
      "   426     13278       7764.0      0.6      0.3      try:\n",
      "   427     13278      13906.0      1.0      0.5          circuit =  dc.utilities.runWithTimeout(\n",
      "   428     13278      12040.0      0.9      0.4              lambda: p.evaluate([])(n_qubits),\n",
      "   429     13278    1322929.0     99.6     48.9              timeout=timeout\n",
      "   430                                                   )\n",
      "   431      6124    1335440.0    218.1     49.4          return state_circuit_to_mat(circuit)\n",
      "   432      7154       8300.0      1.2      0.3      except dc.utilities.RunWithTimeout: return None\n",
      "   433      7154       3814.0      0.5      0.1      except: return None\n",
      "\n",
      "Total time: 5.14877 s\n",
      "File: /Users/lsarra/ownCloud/topics/artificial-scientific-discovery/2021_Unitary-Synthesis/ec/dreamcoder/domains/quantum_algorithms/tasks.py\n",
      "Function: logLikelihood at line 21\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    21                                               def logLikelihood(self, e, timeout=None):\n",
      "    22    203252     132890.0      0.7      2.6          if QuantumTask.last_algorithm is not e:\n",
      "    23     12152      15143.0      1.2      0.3              QuantumTask.last_algorithm = e\n",
      "    24     12152      14729.0      1.2      0.3              QuantumTask.last_algorithm_evaluations = {}\n",
      "    25                                           \n",
      "    26    204620     205369.0      1.0      4.0          for n in range(self.min_size, self.max_size):\n",
      "    27    204378     162548.0      0.8      3.2              if n not in QuantumTask.last_algorithm_evaluations.keys():\n",
      "    28     13278    2797987.0    210.7     54.3                  QuantumTask.last_algorithm_evaluations[n] = execute_quantum_algorithm(e, n, timeout)\n",
      "    29                                           \n",
      "    30    204378     112376.0      0.5      2.2              yh = QuantumTask.last_algorithm_evaluations[n]\n",
      "    31    204378     153265.0      0.7      3.0              yh_true = self.target_algorithm_evaluations[n]\n",
      "    32                                           \n",
      "    33    204378      94276.0      0.5      1.8              if yh is None:\n",
      "    34    119090      65833.0      0.6      1.3                  return dc.utilities.NEGATIVEINFINITY\n",
      "    35                                                       \n",
      "    36     85288    1318634.0     15.5     25.6              if not np.all(np.abs(yh-yh_true)<= 1e-5):\n",
      "    37     83920      75608.0      0.9      1.5                  return dc.utilities.NEGATIVEINFINITY\n",
      "    38                                                           \n",
      "    39       242        112.0      0.5      0.0          return 0.\n",
      "\n",
      "Total time: 36.3868 s\n",
      "File: /Users/lsarra/ownCloud/topics/artificial-scientific-discovery/2021_Unitary-Synthesis/ec/dreamcoder/enumeration.py\n",
      "Function: multicoreEnumeration at line 10\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    10                                           def multicoreEnumeration(g, tasks, _=None,\n",
      "    11                                                                    enumerationTimeout=None,\n",
      "    12                                                                    solver='ocaml',\n",
      "    13                                                                    CPUs=1,\n",
      "    14                                                                    maximumFrontier=None,\n",
      "    15                                                                    verbose=True,\n",
      "    16                                                                    evaluationTimeout=None,\n",
      "    17                                                                    testing=False):\n",
      "    18                                               '''g: Either a Grammar, or a map from task to grammar.\n",
      "    19                                               Returns (list-of-frontiers, map-from-task-to-search-time)'''\n",
      "    20                                           \n",
      "    21                                               # We don't use actual threads but instead use the multiprocessing\n",
      "    22                                               # library. This is because we need to be able to kill workers.\n",
      "    23                                               #from multiprocess import Process, Queue\n",
      "    24                                           \n",
      "    25         6        499.0     83.2      0.0      from multiprocessing import Queue\n",
      "    26                                           \n",
      "    27                                                # everything that gets sent between processes will be dilled\n",
      "    28         6         84.0     14.0      0.0      import dill\n",
      "    29                                               \n",
      "    30         6         44.0      7.3      0.0      solvers = {\"ocaml\": solveForTask_ocaml,\n",
      "    31         6         33.0      5.5      0.0                 \"bottom\": solveForTask_bottom,   \n",
      "    32         6         43.0      7.2      0.0                 \"pypy\": solveForTask_pypy,   \n",
      "    33         6        312.0     52.0      0.0                 \"python\": solveForTask_python}   \n",
      "    34         6         29.0      4.8      0.0      assert solver in solvers, \"You must specify a valid solver. options are ocaml, pypy, or python.\" \n",
      "    35                                           \n",
      "    36         6         22.0      3.7      0.0      likelihoodModel = None\n",
      "    37         6         27.0      4.5      0.0      if solver == 'pypy' or solver == 'python':\n",
      "    38                                                 # Use an all or nothing likelihood model.\n",
      "    39         6        422.0     70.3      0.0        likelihoodModel = AllOrNothingLikelihoodModel(timeout=evaluationTimeout) \n",
      "    40                                                 \n",
      "    41                                           \n",
      "    42         6        101.0     16.8      0.0      if not isinstance(g, dict):\n",
      "    43         6        578.0     96.3      0.0          g = {t: g for t in tasks}\n",
      "    44                                               \n",
      "    45                                               \n",
      "    46         6         41.0      6.8      0.0      if solver == \"bottom\":\n",
      "    47                                                   for t, _g in g.items():\n",
      "    48                                                       _g.unrolled = PCFG.from_grammar(_g, t.request).number_rules()\n",
      "    49                                                           \n",
      "    50         6         23.0      3.8      0.0      task2grammar = g\n",
      "    51                                           \n",
      "    52         6         42.0      7.0      0.0      solver_str = solver\n",
      "    53         6         29.0      4.8      0.0      solver = solvers[solver]\n",
      "    54                                           \n",
      "    55                                               # If we are not evaluating on held out testing tasks:\n",
      "    56                                               # Bin the tasks by request type and grammar\n",
      "    57                                               # If these are the same then we can enumerate for multiple tasks simultaneously\n",
      "    58                                               # If we are evaluating testing tasks:\n",
      "    59                                               # Make sure that each job corresponds to exactly one task\n",
      "    60         6         23.0      3.8      0.0      jobs = {}\n",
      "    61       120        467.0      3.9      0.0      for i, t in enumerate(tasks):\n",
      "    62       114        408.0      3.6      0.0          if testing:\n",
      "    63                                                       k = (task2grammar[t], t.request, i)\n",
      "    64                                                   else:\n",
      "    65       114        688.0      6.0      0.0              k = (task2grammar[t], t.request)\n",
      "    66       114      10581.0     92.8      0.0          jobs[k] = jobs.get(k, []) + [t]\n",
      "    67                                           \n",
      "    68         6         40.0      6.7      0.0      disableParallelism = len(jobs) == 1\n",
      "    69         6         21.0      3.5      0.0      parallelCallback = launchParallelProcess if not disableParallelism else lambda f, * \\\n",
      "    70                                                   a, **k: f(*a, **k)\n",
      "    71         6         27.0      4.5      0.0      if disableParallelism:\n",
      "    72         6       7150.0   1191.7      0.0          eprint(\"Disabling parallelism on the Python side because we only have one job.\")\n",
      "    73         6       5959.0    993.2      0.0          eprint(\"If you are using ocaml or bottom, there could still be parallelism.\")\n",
      "    74                                           \n",
      "    75                                               # Map from task to the shortest time to find a program solving it\n",
      "    76         6        221.0     36.8      0.0      bestSearchTime = {t: None for t in task2grammar}\n",
      "    77                                           \n",
      "    78         6        479.0     79.8      0.0      lowerBounds = {k: 0. for k in jobs}\n",
      "    79                                           \n",
      "    80         6        527.0     87.8      0.0      frontiers = {t: Frontier([], task=t) for t in task2grammar}\n",
      "    81                                           \n",
      "    82                                               # For each job we keep track of how long we have been working on it\n",
      "    83         6        480.0     80.0      0.0      stopwatches = {t: Stopwatch() for t in jobs}\n",
      "    84                                           \n",
      "    85                                               # Map from task to how many programs we enumerated for that task\n",
      "    86         6        129.0     21.5      0.0      taskToNumberOfPrograms = {t: 0 for t in tasks }\n",
      "    87                                           \n",
      "    88         6         33.0      5.5      0.0      def numberOfHits(f):\n",
      "    89                                                   return sum(e.logLikelihood > -0.01 for e in f)\n",
      "    90                                           \n",
      "    91         6         32.0      5.3      0.0      def budgetIncrement(lb):\n",
      "    92                                                   nonlocal solver_str\n",
      "    93                                                   if solver_str==\"bottom\":\n",
      "    94                                                       return 6\n",
      "    95                                                   else:\n",
      "    96                                                       return 1.5\n",
      "    97                                           \n",
      "    98         6         34.0      5.7      0.0      def maximumFrontiers(j):\n",
      "    99                                                   tasks = jobs[j]\n",
      "   100                                                   return {t: maximumFrontier - numberOfHits(frontiers[t]) for t in tasks}\n",
      "   101                                           \n",
      "   102         6         19.0      3.2      0.0      def allocateCPUs(n, tasks):\n",
      "   103                                                   allocation = {t: 0 for t in tasks}\n",
      "   104                                                   while n > 0:\n",
      "   105                                                       for t in tasks:\n",
      "   106                                                           # During testing we use exactly one CPU per task\n",
      "   107                                                           if testing and allocation[t] > 0:\n",
      "   108                                                               return allocation\n",
      "   109                                                           allocation[t] += 1\n",
      "   110                                                           n -= 1\n",
      "   111                                                           if n == 0:\n",
      "   112                                                               break\n",
      "   113                                                   return allocation\n",
      "   114                                           \n",
      "   115         6         21.0      3.5      0.0      def refreshJobs():\n",
      "   116                                                   for k in list(jobs.keys()):\n",
      "   117                                                       v = [t for t in jobs[k]\n",
      "   118                                                            if numberOfHits(frontiers[t]) < maximumFrontier\n",
      "   119                                                            and stopwatches[k].elapsed <= enumerationTimeout]\n",
      "   120                                                       if v:\n",
      "   121                                                           jobs[k] = v\n",
      "   122                                                       else:\n",
      "   123                                                           del jobs[k]\n",
      "   124                                           \n",
      "   125                                               # Workers put their messages in here\n",
      "   126         6       6756.0   1126.0      0.0      q = Queue()\n",
      "   127                                           \n",
      "   128                                               # How many CPUs are we using?\n",
      "   129         6         29.0      4.8      0.0      activeCPUs = 0\n",
      "   130                                           \n",
      "   131                                               # How many CPUs was each job allocated?\n",
      "   132         6         22.0      3.7      0.0      id2CPUs = {}\n",
      "   133                                               # What job was each ID working on?\n",
      "   134         6         21.0      3.5      0.0      id2job = {}\n",
      "   135         6         22.0      3.7      0.0      nextID = 0\n",
      "   136                                           \n",
      "   137         6         22.0      3.7      0.0      while True:\n",
      "   138        54      35212.0    652.1      0.1          refreshJobs()\n",
      "   139                                                   # Don't launch a job that we are already working on\n",
      "   140                                                   # We run the stopwatch whenever the job is being worked on\n",
      "   141                                                   # freeJobs are things that we are not working on but could be\n",
      "   142        54       3357.0     62.2      0.0          freeJobs = [j for j in jobs if not stopwatches[j].running\n",
      "   143                                                               and stopwatches[j].elapsed < enumerationTimeout - 0.5]\n",
      "   144        54        206.0      3.8      0.0          if freeJobs and activeCPUs < CPUs:\n",
      "   145                                                       # Allocate a CPU to each of the jobs that we have made the least\n",
      "   146                                                       # progress on\n",
      "   147        48       1753.0     36.5      0.0              freeJobs.sort(key=lambda j: lowerBounds[j])\n",
      "   148                                                       # Launch some more jobs until all of the CPUs are being used\n",
      "   149        48        193.0      4.0      0.0              availableCPUs = CPUs - activeCPUs\n",
      "   150        48      24117.0    502.4      0.1              allocation = allocateCPUs(availableCPUs, freeJobs)\n",
      "   151        96        358.0      3.7      0.0              for j in freeJobs:\n",
      "   152        48       1477.0     30.8      0.0                  if allocation[j] == 0:\n",
      "   153                                                               continue\n",
      "   154        48        200.0      4.2      0.0                  g, request = j[:2]\n",
      "   155        48       1541.0     32.1      0.0                  bi = budgetIncrement(lowerBounds[j])\n",
      "   156        48       2248.0     46.8      0.0                  thisTimeout = enumerationTimeout - stopwatches[j].elapsed\n",
      "   157        48        246.0      5.1      0.0                  eprint(\"(frontend) Launching %s (%d tasks) w/ %d CPUs. %f <= MDL < %f. Timeout %f.\" %\n",
      "   158        48      68289.0   1422.7      0.2                         (request, len(jobs[j]), allocation[j], lowerBounds[j], lowerBounds[j] + bi, thisTimeout))\n",
      "   159        48       2655.0     55.3      0.0                  stopwatches[j].start()\n",
      "   160        48        541.0     11.3      0.0                  parallelCallback(wrapInThread(solver),\n",
      "   161        48        183.0      3.8      0.0                                   q=q, g=g, ID=nextID,\n",
      "   162        48       1815.0     37.8      0.0                                   elapsedTime=stopwatches[j].elapsed,\n",
      "   163        48       1581.0     32.9      0.0                                   CPUs=allocation[j],\n",
      "   164        48       1840.0     38.3      0.0                                   tasks=jobs[j],\n",
      "   165        48       1563.0     32.6      0.0                                   lowerBound=lowerBounds[j],\n",
      "   166        48       1618.0     33.7      0.0                                   upperBound=lowerBounds[j] + bi,\n",
      "   167        48        166.0      3.5      0.0                                   budgetIncrement=bi,\n",
      "   168        48        177.0      3.7      0.0                                   timeout=thisTimeout,\n",
      "   169        48        167.0      3.5      0.0                                   evaluationTimeout=evaluationTimeout,\n",
      "   170        48       4845.0    100.9      0.0                                   maximumFrontiers=maximumFrontiers(j),\n",
      "   171        48        176.0      3.7      0.0                                   testing=testing,\n",
      "   172        48   36032441.0 750675.9     99.0                                   likelihoodModel=likelihoodModel)\n",
      "   173        48       2892.0     60.2      0.0                  id2CPUs[nextID] = allocation[j]\n",
      "   174        48        236.0      4.9      0.0                  id2job[nextID] = j\n",
      "   175        48        194.0      4.0      0.0                  nextID += 1\n",
      "   176                                           \n",
      "   177        48       1749.0     36.4      0.0                  activeCPUs += allocation[j]\n",
      "   178        48       2907.0     60.6      0.0                  lowerBounds[j] += bi\n",
      "   179                                           \n",
      "   180                                                   # If nothing is running, and we just tried to launch jobs,\n",
      "   181                                                   # then that means we are finished\n",
      "   182        54        581.0     10.8      0.0          if all(not s.running for s in stopwatches.values()):\n",
      "   183         6         20.0      3.3      0.0              break\n",
      "   184                                           \n",
      "   185                                                   # Wait to get a response\n",
      "   186        48      81904.0   1706.3      0.2          message = Bunch(dill.loads(q.get()))\n",
      "   187                                           \n",
      "   188        48        232.0      4.8      0.0          if message.result == \"failure\":\n",
      "   189                                                       eprint(\"PANIC! Exception in child worker:\", message.exception)\n",
      "   190                                                       eprint(message.stacktrace)\n",
      "   191                                                       assert False\n",
      "   192        48        170.0      3.5      0.0          elif message.result == \"success\":\n",
      "   193                                                       # Mark the CPUs is no longer being used and pause the stopwatch\n",
      "   194        48        197.0      4.1      0.0              activeCPUs -= id2CPUs[message.ID]\n",
      "   195        48       2370.0     49.4      0.0              stopwatches[id2job[message.ID]].stop()\n",
      "   196                                           \n",
      "   197        48       1880.0     39.2      0.0              newFrontiers, searchTimes, pc = message.value\n",
      "   198       930       3672.0      3.9      0.0              for t, f in newFrontiers.items():\n",
      "   199       882       3080.0      3.5      0.0                  oldBest = None if len(\n",
      "   200       882       6247.0      7.1      0.0                      frontiers[t]) == 0 else frontiers[t].bestPosterior\n",
      "   201       882      14902.0     16.9      0.0                  frontiers[t] = frontiers[t].combine(f)\n",
      "   202       882       3350.0      3.8      0.0                  newBest = None if len(\n",
      "   203       882       6328.0      7.2      0.0                      frontiers[t]) == 0 else frontiers[t].bestPosterior\n",
      "   204                                           \n",
      "   205       882       4746.0      5.4      0.0                  taskToNumberOfPrograms[t] += pc\n",
      "   206                                           \n",
      "   207       882       3599.0      4.1      0.0                  dt = searchTimes[t]\n",
      "   208       882       3140.0      3.6      0.0                  if dt is not None:\n",
      "   209        53        238.0      4.5      0.0                      if bestSearchTime[t] is None:\n",
      "   210        18         85.0      4.7      0.0                          bestSearchTime[t] = dt\n",
      "   211                                                               else:\n",
      "   212                                                                   # newBest & oldBest should both be defined\n",
      "   213        35        113.0      3.2      0.0                          assert oldBest is not None\n",
      "   214        35        111.0      3.2      0.0                          assert newBest is not None\n",
      "   215        35        128.0      3.7      0.0                          newScore = newBest.logPrior + newBest.logLikelihood\n",
      "   216        35        117.0      3.3      0.0                          oldScore = oldBest.logPrior + oldBest.logLikelihood\n",
      "   217                                           \n",
      "   218        35        112.0      3.2      0.0                          if newScore > oldScore:\n",
      "   219                                                                       bestSearchTime[t] = dt\n",
      "   220        35        115.0      3.3      0.0                          elif newScore == oldScore:\n",
      "   221        35        227.0      6.5      0.0                              bestSearchTime[t] = min(bestSearchTime[t], dt)\n",
      "   222                                                   else:\n",
      "   223                                                       eprint(\"Unknown message result:\", message.result)\n",
      "   224                                                       assert False\n",
      "   225                                           \n",
      "   226         6         22.0      3.7      0.0      eprint(\"We enumerated this many programs, for each task:\\n\\t\",\n",
      "   227         6      15539.0   2589.8      0.0             list(taskToNumberOfPrograms.values()))\n",
      "   228                                           \n",
      "   229         6        133.0     22.2      0.0      return [frontiers[t] for t in tasks], bestSearchTime"
     ]
    }
   ],
   "source": [
    "%lprun -f dc.domains.quantum_algorithms.primitives.tensor_contraction -f dc.domains.quantum_algorithms.tasks.QuantumTask.logLikelihood -f dc.domains.quantum_algorithms.primitives.execute_quantum_algorithm -f full_circuit_to_mat -f dc.enumeration.multicoreEnumeration main(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, -13.595880825949859)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = dc.program.Program.parse(\"(lambda ((rep (inc(inc(dec 0))) (lambda (mv $0))) (no_op $0)))\")\n",
    "code.evaluate([])(5)\n",
    "code.infer()\n",
    "task.logLikelihood(code),  grammar.logLikelihood(code.infer(), code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continuationtype = tcircuit\n",
    "#avoid  no _ op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\tt0\t$_\n",
      "0.000000\ttcircuit -> tcircuit\tmv\n",
      "0.000000\ttcircuit -> tcircuit\tmv_r\n",
      "0.000000\ttcircuit -> tcircuit\tminv\n",
      "0.000000\ttsize -> tcircuit\tno_op\n",
      "0.000000\ttcircuit -> tcircuit\th\n",
      "0.000000\ttcircuit -> tcircuit\tcnot\n",
      "0.000000\tint -> (tcircuit -> tcircuit) -> tcircuit -> tcircuit\trep\n",
      "0.000000\tint\t0\n",
      "0.000000\tint -> int\tinc\n",
      "0.000000\tint -> int\tdec\n",
      "0.000000\ttsize -> int\tsize_to_int\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs with and without observational equivalence\n",
    "\n",
    "# primitives = [\n",
    "#     p_move_next,\n",
    "#     p_no_op,\n",
    "#     p_hadamard,\n",
    "# ]\n",
    "\n",
    "\n",
    "# grammar = dc.grammar.Grammar.uniform(primitives)\n",
    "\n",
    "restricted_pcfg = dc.grammar.PCFG.from_grammar(grammar, request=dc.type.arrow(tsize, tcircuit))\n",
    "full_pcfg = dc.grammar.PCFG.from_grammar(full_grammar, request=dc.type.arrow(tsize, tcircuit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = restricted_pcfg.quantized_enumeration(observational_equivalence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lambda (minv (cnot (no_op $0))))\n",
      "(lambda (h (mv (no_op $0))))\n",
      "(lambda (h (mv_r (no_op $0))))\n",
      "(lambda (h (minv (no_op $0))))\n",
      "(lambda (h (h (no_op $0))))\n",
      "(lambda (h (cnot (no_op $0))))\n",
      "(lambda (cnot (mv (no_op $0))))\n",
      "(lambda (cnot (mv_r (no_op $0))))\n",
      "(lambda (cnot (minv (no_op $0))))\n",
      "(lambda (cnot (h (no_op $0))))\n",
      "(lambda (cnot (cnot (no_op $0))))\n",
      "(lambda (rep 0 (lambda $0) (no_op $0)))\n",
      "(lambda (mv (mv (mv (no_op $0)))))\n",
      "(lambda (mv (mv (mv_r (no_op $0)))))\n",
      "(lambda (mv (mv (minv (no_op $0)))))\n",
      "(lambda (mv (mv (h (no_op $0)))))\n",
      "(lambda (mv (mv (cnot (no_op $0)))))\n",
      "(lambda (mv (mv_r (mv (no_op $0)))))\n",
      "(lambda (mv (mv_r (mv_r (no_op $0)))))\n",
      "(lambda (mv (mv_r (minv (no_op $0)))))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 3], [['hadamard', 0], ['hadamard', 0]]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.program.Program.parse(\"(lambda (mv (h (h (no_op $0)))))\").evaluate([])(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = restricted_pcfg.quantized_enumeration(observational_equivalence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lambda (mv (minv (cnot (no_op $0)))))\n",
      "(lambda (mv (h (mv (no_op $0)))))\n",
      "(lambda (mv (h (cnot (no_op $0)))))\n",
      "(lambda (mv (cnot (mv (no_op $0)))))\n",
      "(lambda (mv (cnot (h (no_op $0)))))\n",
      "(lambda (mv_r (h (mv (no_op $0)))))\n",
      "(lambda (mv_r (cnot (mv (no_op $0)))))\n",
      "(lambda (minv (h (mv (no_op $0)))))\n",
      "(lambda (minv (h (cnot (no_op $0)))))\n",
      "(lambda (minv (cnot (mv (no_op $0)))))\n",
      "(lambda (minv (cnot (h (no_op $0)))))\n",
      "(lambda (h (mv (mv (no_op $0)))))\n",
      "(lambda (h (mv (h (no_op $0)))))\n",
      "(lambda (h (mv (cnot (no_op $0)))))\n",
      "(lambda (h (cnot (mv (no_op $0)))))\n",
      "(lambda (h (cnot (h (no_op $0)))))\n",
      "(lambda (cnot (mv (mv (no_op $0)))))\n",
      "(lambda (cnot (mv (minv (no_op $0)))))\n",
      "(lambda (cnot (mv (h (no_op $0)))))\n",
      "(lambda (cnot (mv (cnot (no_op $0)))))\n",
      "(lambda (cnot (h (mv (no_op $0)))))\n",
      "(lambda (cnot (h (cnot (no_op $0)))))\n",
      "(lambda (mv (mv (mv (minv (no_op $0))))))\n",
      "(lambda (mv (mv (mv (h (no_op $0))))))\n",
      "(lambda (mv (mv (mv (cnot (no_op $0))))))\n",
      "(lambda (mv (mv (minv (h (no_op $0))))))\n",
      "(lambda (mv (mv (minv (cnot (no_op $0))))))\n",
      "(lambda (mv (mv (h (mv (no_op $0))))))\n",
      "(lambda (mv (mv (h (cnot (no_op $0))))))\n",
      "(lambda (mv (mv (cnot (mv (no_op $0))))))\n",
      "(lambda (mv (mv (cnot (h (no_op $0))))))\n",
      "(lambda (mv (minv (h (mv (no_op $0))))))\n",
      "(lambda (mv (minv (h (cnot (no_op $0))))))\n",
      "(lambda (mv (minv (cnot (mv (no_op $0))))))\n",
      "(lambda (mv (minv (cnot (h (no_op $0))))))\n",
      "(lambda (mv (h (mv (mv (no_op $0))))))\n",
      "(lambda (mv (h (mv (h (no_op $0))))))\n",
      "(lambda (mv (h (mv (cnot (no_op $0))))))\n",
      "(lambda (mv (h (cnot (mv (no_op $0))))))\n",
      "(lambda (mv (h (cnot (h (no_op $0))))))\n",
      "(lambda (mv (cnot (mv (mv (no_op $0))))))\n",
      "(lambda (mv (cnot (mv (minv (no_op $0))))))\n",
      "(lambda (mv (cnot (mv (h (no_op $0))))))\n",
      "(lambda (mv (cnot (mv (cnot (no_op $0))))))\n",
      "(lambda (mv (cnot (h (mv (no_op $0))))))\n",
      "(lambda (mv (cnot (h (cnot (no_op $0))))))\n",
      "(lambda (mv_r (minv (h (mv (no_op $0))))))\n",
      "(lambda (mv_r (minv (cnot (mv (no_op $0))))))\n",
      "(lambda (mv_r (h (mv (mv (no_op $0))))))\n",
      "(lambda (mv_r (h (mv (h (no_op $0))))))\n",
      "(lambda (mv_r (h (mv (cnot (no_op $0))))))\n",
      "(lambda (mv_r (h (cnot (mv (no_op $0))))))\n",
      "(lambda (mv_r (cnot (mv (mv (no_op $0))))))\n",
      "(lambda (mv_r (cnot (mv (minv (no_op $0))))))\n",
      "(lambda (mv_r (cnot (mv (h (no_op $0))))))\n",
      "(lambda (mv_r (cnot (mv (cnot (no_op $0))))))\n",
      "(lambda (mv_r (cnot (h (mv (no_op $0))))))\n",
      "(lambda (minv (h (mv (mv (no_op $0))))))\n",
      "(lambda (minv (h (mv (h (no_op $0))))))\n",
      "(lambda (minv (h (mv (cnot (no_op $0))))))\n",
      "(lambda (minv (h (cnot (mv (no_op $0))))))\n",
      "(lambda (minv (h (cnot (h (no_op $0))))))\n",
      "(lambda (minv (cnot (mv (mv (no_op $0))))))\n",
      "(lambda (minv (cnot (mv (minv (no_op $0))))))\n",
      "(lambda (minv (cnot (mv (h (no_op $0))))))\n",
      "(lambda (minv (cnot (mv (cnot (no_op $0))))))\n",
      "(lambda (minv (cnot (h (mv (no_op $0))))))\n",
      "(lambda (minv (cnot (h (cnot (no_op $0))))))\n",
      "(lambda (h (mv (mv (mv (no_op $0))))))\n",
      "(lambda (h (mv (mv (h (no_op $0))))))\n",
      "(lambda (h (mv (mv (cnot (no_op $0))))))\n",
      "(lambda (h (mv (h (mv (no_op $0))))))\n",
      "(lambda (h (mv (h (cnot (no_op $0))))))\n",
      "(lambda (h (mv (cnot (mv (no_op $0))))))\n",
      "(lambda (h (mv (cnot (h (no_op $0))))))\n",
      "(lambda (h (cnot (mv (mv (no_op $0))))))\n",
      "(lambda (h (cnot (mv (minv (no_op $0))))))\n",
      "(lambda (h (cnot (mv (h (no_op $0))))))\n",
      "(lambda (h (cnot (mv (cnot (no_op $0))))))\n",
      "(lambda (h (cnot (h (mv (no_op $0))))))\n",
      "(lambda (h (cnot (h (cnot (no_op $0))))))\n",
      "(lambda (cnot (mv (mv (minv (no_op $0))))))\n",
      "(lambda (cnot (mv (mv (h (no_op $0))))))\n",
      "(lambda (cnot (mv (mv (cnot (no_op $0))))))\n",
      "(lambda (cnot (mv (minv (h (no_op $0))))))\n",
      "(lambda (cnot (mv (minv (cnot (no_op $0))))))\n",
      "(lambda (cnot (mv (h (mv (no_op $0))))))\n",
      "(lambda (cnot (mv (h (cnot (no_op $0))))))\n",
      "(lambda (cnot (mv (cnot (mv (no_op $0))))))\n",
      "(lambda (cnot (mv (cnot (h (no_op $0))))))\n",
      "(lambda (cnot (mv_r (h (mv (no_op $0))))))\n",
      "(lambda (cnot (mv_r (cnot (mv (no_op $0))))))\n",
      "(lambda (cnot (minv (h (mv (no_op $0))))))\n",
      "(lambda (cnot (minv (cnot (mv (no_op $0))))))\n",
      "(lambda (cnot (h (mv (mv (no_op $0))))))\n",
      "(lambda (cnot (h (mv (h (no_op $0))))))\n",
      "(lambda (cnot (h (mv (cnot (no_op $0))))))\n",
      "(lambda (cnot (h (cnot (mv (no_op $0))))))\n",
      "(lambda (cnot (h (cnot (h (no_op $0))))))\n",
      "(lambda (mv (mv (mv (minv (h (no_op $0)))))))\n",
      "(lambda (mv (mv (mv (minv (cnot (no_op $0)))))))\n",
      "(lambda (mv (mv (mv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (mv (mv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (mv (minv (h (mv (no_op $0)))))))\n",
      "(lambda (mv (mv (minv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (mv (minv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (mv (minv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (mv (h (mv (h (no_op $0)))))))\n",
      "(lambda (mv (mv (h (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (mv (h (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (mv (h (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (mv (cnot (mv (minv (no_op $0)))))))\n",
      "(lambda (mv (mv (cnot (mv (h (no_op $0)))))))\n",
      "(lambda (mv (mv (cnot (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (mv (cnot (h (mv (no_op $0)))))))\n",
      "(lambda (mv (mv (cnot (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (minv (h (mv (mv (no_op $0)))))))\n",
      "(lambda (mv (minv (h (mv (h (no_op $0)))))))\n",
      "(lambda (mv (minv (h (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (minv (h (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (minv (h (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (mv (mv (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (mv (minv (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (mv (h (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (h (mv (no_op $0)))))))\n",
      "(lambda (mv (minv (cnot (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (h (mv (mv (h (no_op $0)))))))\n",
      "(lambda (mv (h (mv (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (h (mv (h (mv (no_op $0)))))))\n",
      "(lambda (mv (h (mv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (h (mv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (h (mv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (mv (mv (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (mv (minv (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (mv (h (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (h (mv (no_op $0)))))))\n",
      "(lambda (mv (h (cnot (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (mv (minv (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (mv (h (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (minv (h (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (minv (cnot (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (h (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv_r (h (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (mv_r (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (minv (h (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (minv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (h (mv (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (h (mv (h (no_op $0)))))))\n",
      "(lambda (mv (cnot (h (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv (cnot (h (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv (cnot (h (cnot (h (no_op $0)))))))\n",
      "(lambda (mv_r (mv_r (h (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (mv_r (cnot (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (minv (h (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (minv (h (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (minv (h (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (minv (h (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv_r (minv (cnot (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (minv (cnot (mv (minv (no_op $0)))))))\n",
      "(lambda (mv_r (minv (cnot (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (minv (cnot (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (minv (cnot (h (mv (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (h (mv (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv_r (h (mv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv_r (h (cnot (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (h (cnot (mv (minv (no_op $0)))))))\n",
      "(lambda (mv_r (h (cnot (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (h (cnot (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (h (cnot (h (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (mv (minv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (minv (h (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (minv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (h (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (h (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (mv (cnot (h (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (minv (h (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (minv (cnot (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (h (mv (mv (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (h (mv (h (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (h (mv (cnot (no_op $0)))))))\n",
      "(lambda (mv_r (cnot (h (cnot (mv (no_op $0)))))))\n",
      "(lambda (minv (h (mv (mv (mv (no_op $0)))))))\n",
      "(lambda (minv (h (mv (mv (h (no_op $0)))))))\n",
      "(lambda (minv (h (mv (mv (cnot (no_op $0)))))))\n",
      "(lambda (minv (h (mv (h (mv (no_op $0)))))))\n",
      "(lambda (minv (h (mv (h (cnot (no_op $0)))))))\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enumerating arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives = [\n",
    "    # p_0,\n",
    "    p_inc,\n",
    "    p_dec,\n",
    "]\n",
    "\n",
    "grammar = dc.grammar.Grammar.uniform(primitives)\n",
    "pcfg = dc.grammar.PCFG.from_grammar(grammar, request=dc.type.arrow(dc.type.tint, dc.type.tint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = pcfg.quantized_enumeration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[(lambda 0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lambda $0)\n",
      "(lambda (inc $0))\n",
      "(lambda (dec $0))\n",
      "(lambda (inc (inc $0)))\n",
      "(lambda (inc (dec $0)))\n",
      "(lambda (dec (inc $0)))\n",
      "(lambda (dec (dec $0)))\n",
      "(lambda (inc (inc (inc $0))))\n",
      "(lambda (inc (inc (dec $0))))\n",
      "(lambda (inc (dec (inc $0))))\n",
      "(lambda (inc (dec (dec $0))))\n",
      "(lambda (dec (inc (inc $0))))\n",
      "(lambda (dec (inc (dec $0))))\n",
      "(lambda (dec (dec (inc $0))))\n",
      "(lambda (dec (dec (dec $0))))\n",
      "(lambda (inc (inc (inc (inc $0)))))\n",
      "(lambda (inc (inc (inc (dec $0)))))\n",
      "(lambda (inc (inc (dec (inc $0)))))\n",
      "(lambda (inc (inc (dec (dec $0)))))\n",
      "(lambda (inc (dec (inc (inc $0)))))\n",
      "(lambda (inc (dec (inc (dec $0)))))\n",
      "(lambda (inc (dec (dec (inc $0)))))\n",
      "(lambda (inc (dec (dec (dec $0)))))\n",
      "(lambda (dec (inc (inc (inc $0)))))\n",
      "(lambda (dec (inc (inc (dec $0)))))\n",
      "(lambda (dec (inc (dec (inc $0)))))\n",
      "(lambda (dec (inc (dec (dec $0)))))\n",
      "(lambda (dec (dec (inc (inc $0)))))\n",
      "(lambda (dec (dec (inc (dec $0)))))\n",
      "(lambda (dec (dec (dec (inc $0)))))\n",
      "(lambda (dec (dec (dec (dec $0)))))\n",
      "(lambda (inc (inc (inc (inc (inc $0))))))\n",
      "(lambda (inc (inc (inc (inc (dec $0))))))\n",
      "(lambda (inc (inc (inc (dec (inc $0))))))\n",
      "(lambda (inc (inc (inc (dec (dec $0))))))\n",
      "(lambda (inc (inc (dec (inc (inc $0))))))\n",
      "(lambda (inc (inc (dec (inc (dec $0))))))\n",
      "(lambda (inc (inc (dec (dec (inc $0))))))\n",
      "(lambda (inc (inc (dec (dec (dec $0))))))\n",
      "(lambda (inc (dec (inc (inc (inc $0))))))\n",
      "(lambda (inc (dec (inc (inc (dec $0))))))\n",
      "(lambda (inc (dec (inc (dec (inc $0))))))\n",
      "(lambda (inc (dec (inc (dec (dec $0))))))\n",
      "(lambda (inc (dec (dec (inc (inc $0))))))\n",
      "(lambda (inc (dec (dec (inc (dec $0))))))\n",
      "(lambda (inc (dec (dec (dec (inc $0))))))\n",
      "(lambda (inc (dec (dec (dec (dec $0))))))\n",
      "(lambda (dec (inc (inc (inc (inc $0))))))\n",
      "(lambda (dec (inc (inc (inc (dec $0))))))\n",
      "(lambda (dec (inc (inc (dec (inc $0))))))\n",
      "(lambda (dec (inc (inc (dec (dec $0))))))\n",
      "(lambda (dec (inc (dec (inc (inc $0))))))\n",
      "(lambda (dec (inc (dec (inc (dec $0))))))\n",
      "(lambda (dec (inc (dec (dec (inc $0))))))\n",
      "(lambda (dec (inc (dec (dec (dec $0))))))\n",
      "(lambda (dec (dec (inc (inc (inc $0))))))\n",
      "(lambda (dec (dec (inc (inc (dec $0))))))\n",
      "(lambda (dec (dec (inc (dec (inc $0))))))\n",
      "(lambda (dec (dec (inc (dec (dec $0))))))\n",
      "(lambda (dec (dec (dec (inc (inc $0))))))\n",
      "(lambda (dec (dec (dec (inc (dec $0))))))\n",
      "(lambda (dec (dec (dec (dec (inc $0))))))\n",
      "(lambda (dec (dec (dec (dec (dec $0))))))\n",
      "(lambda (inc (inc (inc (inc (inc (inc $0)))))))\n",
      "(lambda (inc (inc (inc (inc (inc (dec $0)))))))\n",
      "(lambda (inc (inc (inc (inc (dec (inc $0)))))))\n",
      "(lambda (inc (inc (inc (inc (dec (dec $0)))))))\n",
      "(lambda (inc (inc (inc (dec (inc (inc $0)))))))\n",
      "(lambda (inc (inc (inc (dec (inc (dec $0)))))))\n",
      "(lambda (inc (inc (inc (dec (dec (inc $0)))))))\n",
      "(lambda (inc (inc (inc (dec (dec (dec $0)))))))\n",
      "(lambda (inc (inc (dec (inc (inc (inc $0)))))))\n",
      "(lambda (inc (inc (dec (inc (inc (dec $0)))))))\n",
      "(lambda (inc (inc (dec (inc (dec (inc $0)))))))\n",
      "(lambda (inc (inc (dec (inc (dec (dec $0)))))))\n",
      "(lambda (inc (inc (dec (dec (inc (inc $0)))))))\n",
      "(lambda (inc (inc (dec (dec (inc (dec $0)))))))\n",
      "(lambda (inc (inc (dec (dec (dec (inc $0)))))))\n",
      "(lambda (inc (inc (dec (dec (dec (dec $0)))))))\n",
      "(lambda (inc (dec (inc (inc (inc (inc $0)))))))\n",
      "(lambda (inc (dec (inc (inc (inc (dec $0)))))))\n",
      "(lambda (inc (dec (inc (inc (dec (inc $0)))))))\n",
      "(lambda (inc (dec (inc (inc (dec (dec $0)))))))\n",
      "(lambda (inc (dec (inc (dec (inc (inc $0)))))))\n",
      "(lambda (inc (dec (inc (dec (inc (dec $0)))))))\n",
      "(lambda (inc (dec (inc (dec (dec (inc $0)))))))\n",
      "(lambda (inc (dec (inc (dec (dec (dec $0)))))))\n",
      "(lambda (inc (dec (dec (inc (inc (inc $0)))))))\n",
      "(lambda (inc (dec (dec (inc (inc (dec $0)))))))\n",
      "(lambda (inc (dec (dec (inc (dec (inc $0)))))))\n",
      "(lambda (inc (dec (dec (inc (dec (dec $0)))))))\n",
      "(lambda (inc (dec (dec (dec (inc (inc $0)))))))\n",
      "(lambda (inc (dec (dec (dec (inc (dec $0)))))))\n",
      "(lambda (inc (dec (dec (dec (dec (inc $0)))))))\n",
      "(lambda (inc (dec (dec (dec (dec (dec $0)))))))\n",
      "(lambda (dec (inc (inc (inc (inc (inc $0)))))))\n",
      "(lambda (dec (inc (inc (inc (inc (dec $0)))))))\n",
      "(lambda (dec (inc (inc (inc (dec (inc $0)))))))\n",
      "(lambda (dec (inc (inc (inc (dec (dec $0)))))))\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in iterator:\n",
    "    counter +=1\n",
    "    if counter<100:\n",
    "        print(i)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start symbol: (tcircuit, (tsize,))\n",
      "\n",
      "(tcircuit, (tsize,)) ::= mv\t0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= mv_r\t0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= minv\t0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= no_op\t0x(tsize, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= h\t0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= cnot\t0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "(tcircuit, (tsize,)) ::= rep\t0x(int, (tsize,)) 1x(tcircuit, (tcircuit, tsize)) 0x(tcircuit, (tsize,))\t\t-1.9459101490553132\n",
      "\n",
      "(tsize, (tsize,)) ::= $0\t\t\t0.0\n",
      "\n",
      "(int, (tsize,)) ::= 0\t\t\t-1.3862943611198906\n",
      "(int, (tsize,)) ::= inc\t0x(int, (tsize,))\t\t-1.3862943611198906\n",
      "(int, (tsize,)) ::= dec\t0x(int, (tsize,))\t\t-1.3862943611198906\n",
      "(int, (tsize,)) ::= size_to_int\t0x(tsize, (tsize,))\t\t-1.3862943611198906\n",
      "\n",
      "(tcircuit, (tcircuit, tsize)) ::= mv\t0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= mv_r\t0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= minv\t0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= no_op\t0x(tsize, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= h\t0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= cnot\t0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= rep\t0x(int, (tcircuit, tsize)) 1x(tcircuit, (tcircuit, tcircuit, tsize)) 0x(tcircuit, (tcircuit, tsize))\t\t-2.0794415416798357\n",
      "(tcircuit, (tcircuit, tsize)) ::= $0\t\t\t-2.0794415416798357\n",
      "\n",
      "(tsize, (tcircuit, tsize)) ::= $1\t\t\t0.0\n",
      "\n",
      "(int, (tcircuit, tsize)) ::= 0\t\t\t-1.3862943611198906\n",
      "(int, (tcircuit, tsize)) ::= inc\t0x(int, (tcircuit, tsize))\t\t-1.3862943611198906\n",
      "(int, (tcircuit, tsize)) ::= dec\t0x(int, (tcircuit, tsize))\t\t-1.3862943611198906\n",
      "(int, (tcircuit, tsize)) ::= size_to_int\t0x(tsize, (tcircuit, tsize))\t\t-1.3862943611198906\n",
      "\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= mv\t0x(tcircuit, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= mv_r\t0x(tcircuit, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= minv\t0x(tcircuit, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= no_op\t0x(tsize, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= h\t0x(tcircuit, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= cnot\t0x(tcircuit, (tcircuit, tcircuit, tsize))\t\t-1.9459101490553132\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= $0\t\t\t-2.6390573296152584\n",
      "(tcircuit, (tcircuit, tcircuit, tsize)) ::= $1\t\t\t-2.6390573296152584\n",
      "\n",
      "(tsize, (tcircuit, tcircuit, tsize)) ::= $2\t\t\t0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[(lambda 0)]\n",
      "Enumerated 6124 programs\n",
      "[(lambda 0)]\n",
      "Enumerated 6656 programs\n"
     ]
    }
   ],
   "source": [
    "restricted_pcfg = dc.grammar.PCFG.from_grammar(grammar, request=dc.type.arrow(tsize, tcircuit))\n",
    "# print(restricted_pcfg)\n",
    "restricted_dictionary = dc.enumeration.enumerate_pcfg(restricted_pcfg,timeout=60, circuit_execution_function=state_circuit_to_mat)\n",
    "\n",
    "full_pcfg = dc.grammar.PCFG.from_grammar(full_grammar, request=dc.type.arrow(tsize, tcircuit_full))\n",
    "full_dictionary = dc.enumeration.enumerate_pcfg(full_pcfg,timeout=60, circuit_execution_function=full_circuit_to_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enumerated 2226 programs\n"
     ]
    }
   ],
   "source": [
    "matched_programs = []\n",
    "for unitary in full_dictionary.keys():\n",
    "    if unitary in restricted_dictionary.keys():\n",
    "        try:\n",
    "            full_task = full_dictionary[unitary][\"task\"]\n",
    "            full_unitary = full_circuit_to_mat(dc.program.Program.parse(full_task).evaluate([])(4))\n",
    "            \n",
    "            restricted_task = restricted_dictionary[unitary][\"task\"]\n",
    "            restricted_unitary = state_circuit_to_mat(dc.program.Program.parse(restricted_task).evaluate([])(4))\n",
    "\n",
    "            if np.all(full_unitary==restricted_unitary):\n",
    "                matched_programs.append([full_task, \n",
    "                                         restricted_task, \n",
    "                                         max(full_dictionary[unitary][\"time\"],restricted_dictionary[unitary][\"time\"])])\n",
    "        except QuantumCircuitException:\n",
    "            ...\n",
    "eprint(f\"Enumerated {len(matched_programs)} programs\")\n",
    "# how long it took to enumerate (when the program was found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "save_path = os.path.join(\"experimentOutputs/quantum/\",\"matched_programs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,\"wb\") as f:\n",
    "    pickle.dump(matched_programs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path,\"rb\") as f:\n",
    "        matched_programs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, []]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.program.Program.parse(matched_programs[0][0]).evaluate([])(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor for the recognition network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class BagOfWordsFeatureExtractor(nn.Module):\n",
    "    def __init__(self, tasks, full_op_names): # why do we need tasks?\n",
    "        super(BagOfWordsFeatureExtractor, self).__init__()\n",
    "        self.recomputeTasks = False\n",
    "        \n",
    "        self.qubit_test_range = [3,5]\n",
    "        self.qubit_num = self.qubit_test_range[1]-self.qubit_test_range[0]+1\n",
    "        \n",
    "        self.names = list(full_op_names.keys())\n",
    "        self.len_names =len(self.names)\n",
    "        \n",
    "        self.outputDimensionality = self.len_names*self.qubit_num\n",
    "        self.tasks=tasks\n",
    "        \n",
    "    # full_circuit to embedding (bag of words)\n",
    "    def full_circuit_to_embedding(self, full_circuit):\n",
    "        embedding = np.zeros([self.len_names], dtype=int)\n",
    "        for operation in full_circuit:\n",
    "            embedding[self.names.index(operation[0])]+=1\n",
    "        return embedding\n",
    "\n",
    "    def full_task_to_embedding(self,full_task):\n",
    "        full_embedding = np.hstack(\n",
    "            [self.full_circuit_to_embedding(full_task.target_algorithm(n_qubit)[1]) \n",
    "             for n_qubit in range(self.qubit_test_range[0],self.qubit_test_range[1]+1)]\n",
    "            )\n",
    "        return full_embedding\n",
    "    \n",
    "    def featuresOfTask(self, t):\n",
    "        return dc.recognition.variable(self.full_task_to_embedding(t)).float()\n",
    "    def featuresOfTasks(self, ts):\n",
    "        return dc.recognition.variable([self.full_task_to_embedding(t) for t in ts]).float()\n",
    "    \n",
    "    def taskOfProgram(self, p, t): # why do we need this?\n",
    "        return dc.task.Task(\"dummy task\", t, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = BagOfWordsFeatureExtractor(None, full_op_names)\n",
    "recognition_model = dc.recognition.RecognitionModel(feature_extractor, grammar, contextual=True)\n",
    "lr=0.000001\n",
    "optimizer = torch.optim.Adam(recognition_model.parameters(), lr=lr, eps=1e-3, amsgrad=True)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31019/31019 [03:41<00:00, 140.09it/s]\n",
      "100%|██████████| 31019/31019 [03:37<00:00, 142.32it/s]\n",
      "100%|██████████| 31019/31019 [03:37<00:00, 142.53it/s]\n",
      "100%|██████████| 31019/31019 [03:38<00:00, 142.25it/s]\n",
      "100%|██████████| 31019/31019 [03:30<00:00, 147.10it/s]\n",
      "100%|██████████| 31019/31019 [03:25<00:00, 150.62it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.51it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.37it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 149.95it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.24it/s]\n",
      "100%|██████████| 31019/31019 [03:28<00:00, 148.82it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.43it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.35it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.01it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.21it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.29it/s]\n",
      "100%|██████████| 31019/31019 [03:26<00:00, 150.23it/s]\n",
      "100%|██████████| 31019/31019 [03:49<00:00, 135.04it/s]\n",
      "100%|██████████| 31019/31019 [04:06<00:00, 125.66it/s]\n",
      "100%|██████████| 31019/31019 [03:55<00:00, 131.86it/s]\n",
      " 51%|█████     | 15823/31019 [02:08<02:03, 123.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g6/m3rq3pbs7lq6drdpnnfm1fvjwthg2w/T/ipykernel_9274/863715773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dc/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dc/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for _ in range(100):\n",
    "    for matched_program in tqdm(matched_programs):\n",
    "        i = np.random.randint(0, len(matched_programs))\n",
    "        task = QuantumTask(\"generated_task\", lambda n_qubit:dc.program.Program.parse(matched_programs[i][0]).evaluate([])(n_qubit))\n",
    "        embedding = recognition_model.featureExtractor.featuresOfTask(task)\n",
    "        \n",
    "        simple_program = dc.program.Program.parse(matched_programs[i][1])\n",
    "        summary = grammar.closedLikelihoodSummary(simple_program.infer(),simple_program)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recognition_model.zero_grad()\n",
    "        \n",
    "        feature = recognition_model._MLP(embedding)\n",
    "        features = feature.expand(1, feature.size(-1))\n",
    "        lls = recognition_model.grammarBuilder.batchedLogLikelihoods(features, [summary])\n",
    "        loss = -lls.max()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8211ff5510>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2709,  0.9316,  0.9572,  0.2493,  1.2643,  1.9371, -3.1956, -0.9628,\n",
       "         0.3095, -0.5367,  0.1847, -0.6924], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognition_model.grammarBuilder.logProductions(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.783640596221253, tensor([-7.1862], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"cnot_10\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda (cnot (minv(mv(no_op $0)))))\")\n",
    "\n",
    "embedding = recognition_model.featureExtractor.featuresOfTask(task)\n",
    "predicted_grammar_of_task = recognition_model(embedding)\n",
    "\n",
    "grammar.logLikelihood(code.infer(),code), predicted_grammar_of_task.logLikelihood(code.infer(),code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.567281192442506, tensor([-13.9487], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"swap_01\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda  (cnot(minv(mv_r(cnot(minv (mv (cnot (no_op $0)))))))))\")\n",
    "\n",
    "embedding = recognition_model.featureExtractor.featuresOfTask(task)\n",
    "predicted_grammar_of_task = recognition_model(embedding)\n",
    "\n",
    "grammar.logLikelihood(code.infer(),code), predicted_grammar_of_task.logLikelihood(code.infer(),code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-52.145060152057745, tensor([-49.6362], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = get_task_from_name(\"swap_0n\",tasks)\n",
    "code = dc.program.Program.parse(\"(lambda ((  (rep (dec(dec(size_to_int $0))) (lambda ((cnot(minv(mv_r(cnot(minv (mv (cnot(mv_r $0)))))))))) )  (mv_r( (rep (dec(size_to_int $0)) (lambda (mv((cnot(minv(mv_r(cnot(minv (mv (cnot $0)))))))))) ) (no_op $0) )))))\")\n",
    "\n",
    "embedding = recognition_model.featureExtractor.featuresOfTask(task)\n",
    "predicted_grammar_of_task = recognition_model(embedding)\n",
    "\n",
    "grammar.logLikelihood(code.infer(),code), predicted_grammar_of_task.logLikelihood(code.infer(),code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# profile running time, enumeration speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_quantum(matched_dictionary={unitaries: simple_program, complicated_list},)\n",
    "# for each sample\n",
    "#     embedding= feature_extractor([unitary, complicated_list]) (i.e. encoder)  #in the case of great we first need an embedding and here we get the final embedding\n",
    "    \n",
    "#     # apply the recognition model\n",
    "#     [from frontierBiasOptimal]\n",
    "#     features = self._MLP(features)\n",
    "#     features = features.expand(batchSize, features.size(-1))  # TODO\n",
    "#     lls = self.grammarBuilder.batchedLogLikelihoods(features, [simple_program])\n",
    "        \n",
    "#     # train (optimize -lls  adam)\n",
    "#     lls.backward\n",
    "    \n",
    "# # look at the new likelihoods\n",
    "#     recognitionmodel.grammarOfTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more enumerated tasks (10k)\n",
    "\n",
    "# bags of words (Gates) e.g. number of occurrences for each gate\n",
    "# great https://github.com/google-research/crossbeam/blob/main/crossbeam/model/great.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observational equivalence\n",
    "# https://cseweb.ucsd.edu/~npolikarpova/publications/oopsla20-probe.pdf\n",
    "\n",
    "# let's start from arithmetic expressions\n",
    "\n",
    "# remove no_op to enable continuation type in grammar\n",
    "\n",
    "# continuationtype: only most recent of this type can be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split (and what happens when only one primitive is allowed)\n",
    "# No_op\n",
    "# Observational equivalence on multiple values\n",
    "# Parallelize training of Feature Extractor network\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalences should depend on non terminal symbol\n",
    "\n",
    "# recognition model should be contestula\n",
    "# remove size to int but we need circuit to size"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3b58914974b8dde835498c747ea4f1aaf3fb4cb185c0609e0c7a19c91a9bce2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('dc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
