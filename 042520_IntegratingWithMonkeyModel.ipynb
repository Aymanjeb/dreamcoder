{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate the human stuff with the monkey stuff into the same modeling pipeline\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from analysis.getModelHumanDists import * \n",
    "from analysis.modelAnalyses import *\n",
    "from analysis.importDrawgood import *\n",
    "from analysis.compareModelHumanGood import *\n",
    "from pythonlib.drawmodel.analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dreamcoder checkpoint\n",
      "Loading dreamcoder tasks\n",
      "DRAW TASK training set: S12\n",
      "DO SHAPING: False\n",
      "training task names:\n",
      "['S12_13_shaping_0', 'S12_13_shaping_1', 'S12_13_shaping_2', 'S12_13_shaping_3', 'S12_13_shaping_4', 'S12_13_shaping_5', 'S12_13_shaping_6', 'S12_13_shaping_7', 'S12_13_shaping_8', 'S12_13_shaping_9', 'S12_13_shaping_10', 'S12_1', 'S12_5', 'S12_8', 'S12_10', 'S12_32', 'S12_34', 'S12_38', 'S12_20', 'S12_39', 'S12_57', 'S12_79', 'S12_113', 'S12_124', 'S12_126', 'S12_133', 'S12_147', 'S12_155', 'S12_163', 'S12_200', 'S12_214', 'S12_31', 'S12_52', 'S12_55', 'S12_70', 'S12_129', 'S12_139', 'S12_141', 'S12_148', 'S12_207', 'S12_222', 'S12_224', 'S12_229', 'S12_233', 'S12_235', 'S12_243', 'S12_246']\n",
      "test tasks:\n",
      "['S12_13_test_1', 'S12_13_test_2', 'S12_13_test_4', 'S12_13_test_5', 'S12_13_test_6', 'S12_13_test_7', 'S12_13_test_8', 'S12_13_test_9', 'S12_13_test_10', 'S12_13_test_11', 'S12_13_test_12', 'S12_132', 'S12_201', 'S12_220', 'S12_247', 'S13_182', 'S13_217', 'S13_219']\n",
      "Loading parses\n",
      "FOUND 65 pre-computed parses!\n",
      "Num dreamcoder tasks 47\n",
      "n supervised tasks 47\n",
      "Num dreamcoder TEST tasks 18\n",
      "n supervised TEST tasks 18\n",
      "did not find any programs across all frontiers and all iterations\n",
      "did not find any programs across all frontiers and all iterations\n",
      "did not find any programs across all frontiers and all iterations\n",
      "did not find any programs across all frontiers and all iterations\n",
      "Loading behavior\n",
      "REMOVING SHAPING STIMULI (THOSE HUMANS WERE NOT GIVEN) [wil even remove things like S9_shaping_5]\n",
      "removing, since I think is shaping: S12_13_shaping_0\n",
      "removing, since I think is shaping: S12_13_shaping_1\n",
      "removing, since I think is shaping: S12_13_shaping_2\n",
      "removing, since I think is shaping: S12_13_shaping_3\n",
      "removing, since I think is shaping: S12_13_shaping_4\n",
      "removing, since I think is shaping: S12_13_shaping_5\n",
      "removing, since I think is shaping: S12_13_shaping_6\n",
      "removing, since I think is shaping: S12_13_shaping_7\n",
      "removing, since I think is shaping: S12_13_shaping_8\n",
      "removing, since I think is shaping: S12_13_shaping_9\n",
      "removing, since I think is shaping: S12_13_shaping_10\n"
     ]
    }
   ],
   "source": [
    "# 1) Extract model parses\n",
    "ECTRAIN=\"S12.10.test5\"\n",
    "DAT = loadCheckpoint(trainset=ECTRAIN, loadparse=True, suppressPrint=True, loadbehavior=True)\n",
    "stimlist = DATgetSolvedStim(DAT, onlyifhasdatflat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trainset', 'result', 'tasks', 'testtasks', 'programnames', 'programnames_test', 'behaviorexpt', 'savedir', 'parses', 'analysavedir', 'summarysavedir', 'loadparse', 'taskresultdict', 'datall_human', 'savedir_datsegs', 'datflatsavedir', 'savedir_modelhudist', 'savedir_parseanalysis'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Prune parses, for analysis purposes.\n",
    "DAT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3d352786d0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrokes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: len() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "strokes = DAT[\"parses\"][0][\"parse\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "4\n",
      "8\n",
      "8\n",
      "16\n",
      "16\n",
      "16\n",
      "8\n",
      "16\n",
      "16\n",
      "96\n",
      "32\n",
      "96\n",
      "48\n",
      "32\n",
      "64\n",
      "128\n",
      "16\n",
      "8\n",
      "16\n",
      "16\n",
      "32\n",
      "96\n",
      "32\n",
      "96\n",
      "96\n",
      "576\n",
      "384\n",
      "128\n",
      "192\n",
      "32\n",
      "64\n",
      "1024\n",
      "1024\n",
      "512\n",
      "128\n",
      "32\n",
      "256\n",
      "256\n",
      "64\n",
      "256\n",
      "128\n",
      "128\n",
      "96\n",
      "192\n",
      "384\n",
      "10000\n",
      "3072\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "for s in stimlist:\n",
    "    datflat = DATloadDatFlat(DAT, s)\n",
    "    print(len(datflat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datsegs = DATloadDatSeg(DAT, stimname)\n",
    "datflat = DATloadDatFlat(DAT, s)\n",
    "datseg = getSegmentation(datflat, unique_codes=True, dosplits=True, removebadstrokes=True, removeLongVertLine=REMOVELL) \n",
    "\n",
    "# 2) Prior: motor efficiency (redo this using continuous parameters?)\n",
    "\n",
    "\n",
    "# 3) Likelihood: redo this using continouos parametres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human\n",
    "DAT[\"datflat_hu\"] = getFlatData(DAT[\"datall_human\"])\n",
    "DAT[\"datseg_hu\"] = getSegmentation(DAT[\"datflat_hu\"], unique_codes=True, dosplits=True, removeLongVertLine=REMOVELL)                                      \n",
    "# model\n",
    "datseg = getSegmentation(datflat, unique_codes=True, dosplits=True, removebadstrokes=True, removeLongVertLine=REMOVELL) # get datseg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all into the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==== VERSION 2 - LOAD ALL AT ONCE INTO MEMEORY, NOT PIECEMEITL\n",
    "# 1) Load dreamcoder\n",
    "# ECTRAINlist = [\"S12.10.test4\", \"S13.10.test4\"]\n",
    "ECTRAINlist = [\"S12.10.test5\", \"S13.10.test5\"]\n",
    "modelkind_list = [\"parse\", \"randomperm\"]\n",
    "ver=\"aggregate\"\n",
    "use_withplannerscore=True\n",
    "\n",
    "distances_flat, DAT_all, workerlist, SAVEDIR = loadMultDCHumanDistances(\n",
    "    ECTRAINlist, modelkind_list, ver, use_withplannerscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## integrate into \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# SUMMARIZE HUMAN-HUMAN DISTS\n",
    "\n",
    "# 1) Collect previously computed human-human distances in a list of dicts (is a full table)\n",
    "outdict = getHumanHumanDists()\n",
    "\n",
    "# 2) Collect all distances, meaned over \"other humans\" grouped by condition.\n",
    "humanlist = [w[\"workerID\"] for w in workerlist]\n",
    "stimlist = [t.name for t in DAT_all[0][\"testtasks\"]]\n",
    "\n",
    "outdict_summary = []\n",
    "for human in humanlist:\n",
    "    print(human)\n",
    "    cond = extractHumanCond(human, workerlist)\n",
    "    # - each human gets one value for each cond\n",
    "    dists_bycond = []\n",
    "    for cond_other in [0,1]:\n",
    "        for stim in stimlist:    \n",
    "            outdict_summary.append({\n",
    "                \"human\":human,\n",
    "                \"cond_human\":cond,\n",
    "                \"cond_other\":cond_other,\n",
    "                \"stim\":stim,\n",
    "                \"dist_mean\":np.mean([o[\"dist\"] for o in outdict if o[\"human1\"]==human and extractHumanCond(o[\"human2\"], workerlist)==cond_other\n",
    "                 and o[\"task\"]==stim])\n",
    "            })\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Get ready for plotting by converting to pandas and aggregating over stim\n",
    "# i.e. first aggregated over other people, and then over stims - shouldn't matter since is ballanced.\n",
    "\n",
    "import pandas as pd\n",
    "from pythonlib.tools import pandastools as pdt\n",
    "\n",
    "df = pd.DataFrame(outdict_summary)\n",
    "df = pdt.aggregGeneral(df, [\"human\", \"cond_other\", \"cond_human\"], [\"dist_mean\"])\n",
    "df = pdt.applyFunctionToAllRows(df, F=lambda x: x[\"cond_human\"]==x[\"cond_other\"], newcolname=\"cond_aligned\")\n",
    "\n",
    "# --- get aligned minus notaligned (df --> dict --> df... is dumb).\n",
    "df_dict = df.to_dict(\"records\")\n",
    "df_dict_minused = []\n",
    "for human in humanlist:\n",
    "    d1 = [d[\"dist_mean\"] for d in df_dict if d[\"human\"]==human and d[\"cond_aligned\"]==True][0]\n",
    "    d2 = [d[\"dist_mean\"] for d in df_dict if d[\"human\"]==human and d[\"cond_aligned\"]==False][0]\n",
    "    \n",
    "    df_dict_minused.append({\n",
    "        \"human\":human,\n",
    "        \"dist_aligned_minus_not\":d1-d2,\n",
    "    })\n",
    "df_aligned = pd.DataFrame(df_dict_minused)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = f\"analysis/summaryfigs/acrossexpt/ecS12.10.test5_S13.10.test5-dg2.4_2.4/closer_analysis/notebook_comparing_human_model_parses\"\n",
    "savedir = f\"{SAVEDIR}/human_human_distances\"\n",
    "import os\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pythonlib.tools.snstools import rotateLabel\n",
    "\n",
    "ax = sns.catplot(x=\"human\", y=\"dist_mean\", hue=\"cond_other\", col=\"cond_human\", data=df, aspect=2, height=4)\n",
    "rotateLabel(ax)\n",
    "ax.savefig(f\"{savedir}/overview_meanOverTeststimAndOtherhuman_byhuman.pdf\")\n",
    "                                                                \n",
    "ax = sns.catplot(x=\"cond_other\", y=\"dist_mean\", hue=\"cond_other\", col=\"cond_human\", data=df)\n",
    "ax.savefig(f\"{savedir}/overview_meanOverTeststimAndOtherhuman_aggreg.pdf\")\n",
    "\n",
    "ax = sns.catplot(x=\"cond_aligned\", y=\"dist_mean\", data=df, ci=68, kind=\"point\")\n",
    "ax.savefig(f\"{savedir}/overview_meanbyaligned.pdf\")\n",
    "\n",
    "ax = sns.catplot(y=\"dist_aligned_minus_not\", data=df_aligned, ci=68, kind=\"point\")\n",
    "plt.ylim([0, -0.04])\n",
    "ax.savefig(f\"{savedir}/overview_grandmean.pdf\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCRATCH\n",
    "# load model-human distances preprocessed\n",
    "DATloadModelHuDist(DAT_all[0], \"S12_1\", \"A2VLTSW6CXIUMR\", \"aggregate\", True)\n",
    "\n",
    "# load processed datsegs (segmented)\n",
    "DATloadDatSeg(DAT_all[0], \"S12_1\")[0]\n",
    "\n",
    "# load stroke data\n",
    "DATloadDatFlat(DAT_all[0], \"S12_1\")[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
